<p align="center">
  ğŸš€ <strong>Reinforcement Learning Assignment 4</strong><br>
  <em>TD3, PPO, and SAC on Continuous-Action Environments</em><br>
  LunarLander-v3 & CarRacing-v3
</p>

<p align="center">
  <a href="https://www.python.org/">
    <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white">
  </a>
  <a href="https://pytorch.org/">
    <img src="https://img.shields.io/badge/PyTorch-2.x-EE4C2C?logo=pytorch&logoColor=white">
  </a>
  <a href="https://wandb.ai/">
    <img src="https://img.shields.io/badge/Weights_&_Biases-Tracking-orange?logo=weightsandbiases&logoColor=white">
  </a>
  <a href="https://huggingface.co/">
    <img src="https://img.shields.io/badge/HuggingFace-Model_Sharing-yellow?logo=huggingface&logoColor=white">
  </a>
  <a href="https://gymnasium.farama.org/">
    <img src="https://img.shields.io/badge/Gymnasium-Box2D-purple?logo=OpenAI&logoColor=white">
  </a>
  <img src="https://img.shields.io/badge/License-MIT-green">
</p>



Ctrl + Shift + V  to view the readme 


# ğŸ“˜ Overview

This project implements **three state-of-the-art model-free RL algorithms** â€” **TD3**, **PPO**, and **SAC** â€” trained on two continuous-control environments:

* **ğŸŒ• LunarLander-v3 (continuous=True)**
* **ğŸï¸ CarRacing-v3 (continuous=True)**

The system includes:

* Full PyTorch implementations
* Custom environment wrappers
* Replay buffers & noise modules
* Weights & Biases experiment tracking
* Automatic video recording of agents
* Hugging Face leaderboard submission
* Reproducible training scripts

---

# ğŸ“‚ Project Structure

```
rl-assignment4/
â”‚
â”œâ”€â”€ algorithms/
â”‚   â”œâ”€â”€ td3/
â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â”œâ”€â”€ actor.py
â”‚   â”‚   â”œâ”€â”€ critic.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ ppo/
â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â”œâ”€â”€ policy.py
â”‚   â”‚   â”œâ”€â”€ value_network.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ sac/
â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â”œâ”€â”€ actor.py
â”‚   â”‚   â”œâ”€â”€ critic.py
â”‚   â”‚   â”œâ”€â”€ temperature.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ replay_buffer.py
â”‚   â”œâ”€â”€ noise.py
â”‚   â”œâ”€â”€ normalization.py
â”‚   â”œâ”€â”€ utils.py
â”‚
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ make_env.py
â”‚   â”œâ”€â”€ wrappers_lunarlander.py
â”‚   â”œâ”€â”€ wrappers_carracing.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_td3.py
â”‚   â”œâ”€â”€ train_ppo.py
â”‚   â”œâ”€â”€ train_sac.py
â”‚   â”œâ”€â”€ eval_agent.py
â”‚
â”œâ”€â”€ huggingface/
â”‚   â”œâ”€â”€ push_td3.py
â”‚   â”œâ”€â”€ push_ppo.py
â”‚   â”œâ”€â”€ push_sac.py
â”‚   â””â”€â”€ model_cards/
â”‚       â”œâ”€â”€ td3_card.md
â”‚       â”œâ”€â”€ ppo_card.md
â”‚       â””â”€â”€ sac_card.md
â”‚
â”œâ”€â”€ saved_models/
â”‚   â”œâ”€â”€ td3/
â”‚   â”œâ”€â”€ ppo/
â”‚   â””â”€â”€ sac/
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ pdf/
â”‚   â””â”€â”€ figures/
â”‚
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ td3_lunarlander.mp4
â”‚   â””â”€â”€ sac_carracing.mp4
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---


### Dependencies include:

* PyTorch
* Gymnasium + Box2D
* NumPy
* Weights & Biases
* HuggingFace Hub
* OpenCV (optional)

---

# ğŸ•¹ï¸ Supported Environments

## ğŸŒ• **LunarLander-v3 (Continuous Mode)**

Action space:

```
[ main engine throttle, side engine throttle ]
```

## ğŸï¸ **CarRacing-v3**

Action space:

```
[ steering (-1..+1), gas (0..1), brake (0..1) ]
```

âœ” High-dimensional observation (96Ã—96 RGB)
âœ” Custom wrappers included (grayscale, frame skip, resize)

---

# ğŸ§  Algorithms Implemented

### âœ” **TD3 â€” Twin Delayed Deep Deterministic Policy Gradient**

* Twin critics (Q1, Q2)
* Policy smoothing noise
* Delayed policy updates
* Target networks
* Replay buffer

### âœ” **PPO â€” Proximal Policy Optimization**

* Clipped objective
* Generalized Advantage Estimation (GAE)
* On-policy rollout buffer
* Mini-batch optimization

### âœ” **SAC â€” Soft Actor-Critic**

* Stochastic Gaussian actor
* Maximum entropy objective
* Automatic temperature tuning (Î±)
* Twin critics
* Replay buffer

---

# ğŸ‹ï¸ Training the Agents

### Train **TD3**

```bash
python training/train_td3.py --env lunarlander
python training/train_td3.py --env carracing
```

### Train **PPO**

```bash
python training/train_ppo.py --env lunarlander
python training/train_ppo.py --env carracing
```

### Train **SAC**

```bash
python training/train_sac.py --env lunarlander
python training/train_sac.py --env carracing
```

All training scripts include:

* Weights & Biases logging
* Automatic evaluation
* Checkpoint saving
* Video generation
* Configurable hyperparameters

---

# ğŸ¥ Recording Videos of Trained Agents

Run evaluation with recording enabled:

```bash
python training/eval_agent.py --algo td3 --env lunarlander --record
```

Videos are saved into:

```
videos/
```

And automatically logged to W&B:

```python
wandb.log({"eval_video": wandb.Video(video_path)})
```

---

# ğŸ“Š Experiment Tracking â€” Weights & Biases

Training scripts log:

* Episode return
* Loss values (actor, critic, value function)
* Q1/Q2 critic estimates
* Entropy & Î± (for SAC)
* Evaluation metrics
* Videos of trained agents

### Creating Your W&B Report

1. Open your W&B project
2. Click **Reports â†’ Create Report**
3. Add:

   * Learning curves
   * Comparison charts
   * Videos
4. Copy the public share link
5. Add it to your final PDF report

---

# ğŸ¤— Hugging Face Leaderboard Submission

Each algorithm has its own upload script:

```bash
python huggingface/push_td3.py
python huggingface/push_ppo.py
python huggingface/push_sac.py
```

Uploads include:

* Model weights
* Model card (markdown)
* Evaluation video
* Training summary

You can see your submission under **hf.co/your-username**.

---

# âœ” Deliverables Checklist

| Deliverable                      | Status |
| -------------------------------- | ------ |
| GitHub repository with full code | âœ…      |
| Video of trained agent           | âœ…      |
| W&B experiment charts            | âœ…      |
| W&B report link                  | âœ…      |
| HuggingFace submission           | âœ…      |
| Final PDF report                 | âœ…      |

---

# ğŸ‘¥ Team Roles

| Member   | Algorithm | Environments           |
| -------- | --------- | ---------------------- |
| Maryam Habeb | TD3       | LunarLander, CarRacing |
| Aya Ayman | PPO       | LunarLander, CarRacing |
| Ziad Asar| SAC       | LunarLander, CarRacing |


